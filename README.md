# Loyalty Flink Consumer

> **Part of Loyalty System:** ÄÃ¢y lÃ  Apache Flink streaming job (Stream Processor) trong há»‡ thá»‘ng 3-tier architecture.
> - **loyalty-infra** - Infrastructure (xem [../loyalty-infra](../loyalty-infra))
> - **loyalty-service** - Spring Boot app (xem [../loyalty-service](../loyalty-service))
> - **loyalty-flink-consumer** - Job nÃ y

Apache Flink consumer job cho xá»­ lÃ½ loyalty check-in events tá»« Kafka vá»›i exactly-once semantics.

## ğŸš€ Tech Stack

- **Java 17** (target, Ä‘á»ƒ cháº¡y trÃªn Flink 1.18.1 Java 17 containers)
- **Apache Flink 1.18.1**
- **Kafka** - Source (`loyalty.checkin.raw`) vÃ  Sink (`loyalty.point.transaction`)
- **MySQL 8.0** - Query Ä‘á»ƒ dedup vÃ  calculate (khÃ´ng ghi trá»±c tiáº¿p)
- **Jackson** - JSON serialization/deserialization

## ğŸ“‹ TÃ­nh nÄƒng

### Architecture: Kafka â†’ Flink â†’ Kafka

```
loyalty.checkin.raw (Kafka)
         â†“
    Flink Job
  - Consume events
  - Dedup (state + DB check)
  - Apply business rule (points)
  - Calculate checkin order
         â†“
loyalty.point.transaction (Kafka)
         â†“
  loyalty-service (DB Writer)
         â†“
      MySQL
```

### 1. Kafka Source
- Consumer tá»« topic `loyalty.checkin.raw`
- Exactly-once semantics vá»›i isolation level `read_committed`
- Deduplication theo `eventId` (MapState + DB check)

### 2. Stream Processing
- **KeyBy userId** Ä‘á»ƒ process events per user
- **Dedup layer**: MapState (TTL 7 days) + DB check
- **Business logic**: Calculate checkin order, apply points rule
- **Output**: `PointTransactionEvent` vá»›i transactionId unique

### 3. Kafka Sink (Transactional)
- Publish `PointTransactionEvent` lÃªn topic `loyalty.point.transaction`
- Kafka transactional producer (`DeliveryGuarantee.EXACTLY_ONCE`)
- Transactional ID prefix: `flink-loyalty-tx-`
- **Key by userId**: Ensures same user events â†’ same partition â†’ prevents deadlock in DB Writer

### 4. Checkpoint & State
- Checkpoint interval: 60 seconds
- State backend: Hashmap (in-memory, rollback version - khÃ´ng dÃ¹ng RocksDB)
- Checkpoint storage: Local filesystem (`file:///tmp/flink-checkpoints`)
- Savepoint support for upgrades

## ğŸ—ï¸ Cáº¥u trÃºc

```
loyalty-flink-consumer/
â”œâ”€â”€ pom.xml
â”œâ”€â”€ README.md
â”œâ”€â”€ src/
â”‚   â””â”€â”€ main/
â”‚       â”œâ”€â”€ java/vn/ghtk/loyalty/flink/
â”‚       â”‚   â”œâ”€â”€ CheckinEventConsumerJob.java              # Main job (Kafka â†’ Kafka)
â”‚       â”‚   â”œâ”€â”€ model/
â”‚       â”‚   â”‚   â”œâ”€â”€ CheckinEvent.java                     # Input event
â”‚       â”‚   â”‚   â”œâ”€â”€ PointTransactionEvent.java            # Output event
â”‚       â”‚   â”‚   â””â”€â”€ EventMetadata.java                    # Metadata
â”‚       â”‚   â”œâ”€â”€ function/
â”‚       â”‚   â”‚   â”œâ”€â”€ CheckinEventDeserializer.java         # Kafka deserializer
â”‚       â”‚   â”‚   â”œâ”€â”€ PointTransactionProcessFunction.java  # Dedup + business logic
â”‚       â”‚   â”‚   â””â”€â”€ PointTransactionSerializer.java       # Kafka serializer
â”‚       â”‚   â””â”€â”€ config/
â”‚       â”‚       â””â”€â”€ JobConfig.java                        # Job configuration
â”‚       â””â”€â”€ resources/
â”‚           â””â”€â”€ log4j2.properties
â””â”€â”€ target/
    â””â”€â”€ loyalty-flink-consumer-1.0.0-SNAPSHOT.jar
```

## ğŸ› ï¸ Build

```powershell
mvn clean package -DskipTests
```

Output: `target/loyalty-flink-consumer-1.0.0-SNAPSHOT.jar`

## ğŸš€ Deploy to Flink Cluster

### Prerequisites
- Infrastructure Ä‘Ã£ cháº¡y (xem [../loyalty-infra](../loyalty-infra))
- Kafka topics Ä‘Ã£ Ä‘Æ°á»£c táº¡o

### 1. Copy JAR vÃ o Flink JobManager

```powershell
docker cp target\loyalty-flink-consumer-1.0.0-SNAPSHOT.jar loyalty-flink-jobmanager:/opt/flink/
```

### 2. Submit job (1 dÃ²ng)

```powershell
docker exec -it loyalty-flink-jobmanager flink run -c vn.ghtk.loyalty.flink.CheckinEventConsumerJob -p 4 /opt/flink/loyalty-flink-consumer-1.0.0-SNAPSHOT.jar --kafka.bootstrap.servers kafka:29092 --kafka.topic loyalty.checkin.raw --kafka.group.id loyalty-flink-consumer --kafka.output.topic loyalty.point.transaction --mysql.url jdbc:mysql://mysql:3306/loyalty_db --mysql.username root --mysql.password root --checkpoint.interval 60000 --parallelism 4
```

### 3. Verify job

- **Flink UI**: http://localhost:8081
- **Kafka UI**: http://localhost:8090
  - Check topic `loyalty.checkin.raw` (input)
  - Check topic `loyalty.point.transaction` (output)

## âš™ï¸ Configuration

Job arguments (all optional, cÃ³ defaults):

| Argument | Default | Description |
|----------|---------|-------------|
| `--kafka.bootstrap.servers` | `localhost:9092` | Kafka bootstrap servers |
| `--kafka.topic` | `loyalty.checkin.raw` | Input Kafka topic |
| `--kafka.output.topic` | `loyalty.point.transaction` | Output Kafka topic |
| `--kafka.group.id` | `loyalty-flink-consumer` | Consumer group ID |
| `--mysql.url` | `jdbc:mysql://localhost:3306/loyalty_db` | MySQL JDBC URL (for query only) |
| `--mysql.username` | `root` | MySQL username |
| `--mysql.password` | `root` | MySQL password |
| `--checkpoint.interval` | `60000` | Checkpoint interval (ms) |
| `--parallelism` | `4` | Job parallelism |

## ğŸ“Š Sizing & Performance

### Quy mÃ´ dá»± kiáº¿n

- **1 triá»‡u event/ngÃ y**: ~12 event/giÃ¢y
- **10 triá»‡u event/ngÃ y**: ~116 event/giÃ¢y
- **50 triá»‡u event/ngÃ y**: ~579 event/giÃ¢y

### Sizing cho 1 triá»‡u event/ngÃ y

- **Kafka Partitions**: 4
- **Flink Parallelism**: 4
- **Checkpoint Interval**: 60 seconds
- **State Size**: ~50 MB (1 triá»‡u event IDs * 50 bytes)
- **Memory per TaskManager**: 2 GB
- **CPU per TaskManager**: 2 cores

### Sizing cho 10 triá»‡u event/ngÃ y

- **Kafka Partitions**: 8
- **Flink Parallelism**: 8
- **Checkpoint Interval**: 60 seconds
- **State Size**: ~500 MB (10 triá»‡u event IDs * 50 bytes)
- **Memory per TaskManager**: 4 GB
- **CPU per TaskManager**: 2 cores

## ğŸ”’ Exactly-Once Guarantees

1. **Kafka â†’ Flink**: Flink Kafka connector vá»›i offset commit sau checkpoint
2. **Flink State**: MapState cho deduplication theo eventId (TTL 7 days)
3. **Flink â†’ Kafka**: Kafka transactional sink (`DeliveryGuarantee.EXACTLY_ONCE`)
   - **Partitioning**: Messages keyed by `userId` â†’ same user always in same partition
4. **Kafka â†’ loyalty-service**: Consumer manual commit + DB dedup theo transactionId
   - **Concurrency**: 4 threads (match 4 partitions) â†’ no deadlock vÃ¬ same userId = same thread

**End-to-end exactly-once**:
- API â†’ Kafka (idempotent producer)
- Kafka â†’ Flink (checkpoint + transactional sink)
- Flink â†’ Kafka (transactional)
- Kafka â†’ DB (manual commit + idempotent write)

## ğŸ“ Notes

- **State TTL**: 7 ngÃ y (event IDs auto-expire)
- **Checkpoint storage**: Local (náº¿u muá»‘n persist, dÃ¹ng MinIO/S3)
- **State backend**: Hashmap (in-memory). Náº¿u cáº§n large state, Ä‘á»•i sang RocksDB.
- **MySQL role**: Chá»‰ dÃ¹ng Ä‘á»ƒ query checkin count, khÃ´ng ghi trá»±c tiáº¿p.

## ğŸ› Troubleshooting

### Job failed with checkpoint timeout
- TÄƒng `execution.checkpointing.timeout` trong config
- Giáº£m checkpoint interval náº¿u state quÃ¡ lá»›n

### OutOfMemoryError
- TÄƒng heap memory cho TaskManager
- Enable RocksDB state backend cho large state

### Cannot deserialize event
- Check event schema compatibility giá»¯a producer vÃ  consumer
- Verify Jackson config (LocalDateTime serialization)

### Duplicate transactions in DB
- Check DB Writer deduplication logic
- Verify unique constraint trÃªn `transactionId` hoáº·c `description`

## ğŸ”„ Operations

### Cancel job
```powershell
docker exec -it loyalty-flink-jobmanager flink list
docker exec -it loyalty-flink-jobmanager flink cancel <JOB_ID>
```

### Cancel with savepoint
```powershell
docker exec -it loyalty-flink-jobmanager flink cancel -s file:///tmp/flink-savepoints <JOB_ID>
```

### Resume from savepoint
```powershell
docker exec -it loyalty-flink-jobmanager flink run -s file:///tmp/flink-savepoints/<savepoint-dir> -c vn.ghtk.loyalty.flink.CheckinEventConsumerJob /opt/flink/loyalty-flink-consumer-1.0.0-SNAPSHOT.jar ...
```

## ğŸ“š References

- [Flink Kafka Connector](https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/connectors/datastream/kafka/)
- [Flink Checkpointing](https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/dev/datastream/fault-tolerance/checkpointing/)
- [Flink State & Fault Tolerance](https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/concepts/stateful-stream-processing/)

## ğŸ“„ License

Internal project
