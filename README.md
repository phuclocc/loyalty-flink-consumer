# Loyalty Flink Consumer

Apache Flink consumer job cho xá»­ lÃ½ loyalty check-in events tá»« Kafka vá»›i exactly-once semantics.

## ğŸš€ Tech Stack

- **Java 21**
- **Apache Flink 1.18.1**
- **Kafka** - Source (loyalty.checkin topic)
- **MySQL 8.0** - Sink (exactly-once with 2PC)
- **Jackson** - JSON serialization/deserialization

## ğŸ“‹ TÃ­nh nÄƒng

1. **Kafka Source**
   - Consumer tá»« topic `loyalty.checkin`
   - Exactly-once semantics
   - Deduplication theo `eventId`

2. **Stream Processing**
   - Validate business rules
   - Calculate points based on monthly check-in order
   - Dedup vá»›i MapState (keyed by eventId)

3. **MySQL Sink**
   - 2-Phase Commit (2PC) for exactly-once
   - Atomic writes to 3 tables:
     - `users` - cáº­p nháº­t total_points
     - `daily_checkin` - log check-in record
     - `user_points_history` - log points transaction
     - `checkin_events` - log event for dedup and monitoring

4. **Checkpoint & State**
   - Checkpoint interval: 60 seconds
   - State backend: RocksDB (for large state)
   - Savepoint support for upgrades

## ğŸ—ï¸ Cáº¥u trÃºc

```
loyalty-flink-consumer/
â”œâ”€â”€ pom.xml
â”œâ”€â”€ README.md
â”œâ”€â”€ src/
â”‚   â””â”€â”€ main/
â”‚       â”œâ”€â”€ java/vn/ghtk/loyalty/flink/
â”‚       â”‚   â”œâ”€â”€ CheckinEventConsumerJob.java        # Main job
â”‚       â”‚   â”œâ”€â”€ model/
â”‚       â”‚   â”‚   â”œâ”€â”€ CheckinEvent.java               # Event model
â”‚       â”‚   â”‚   â””â”€â”€ EventMetadata.java              # Metadata model
â”‚       â”‚   â”œâ”€â”€ function/
â”‚       â”‚   â”‚   â”œâ”€â”€ CheckinEventDeserializer.java   # Kafka deserializer
â”‚       â”‚   â”‚   â”œâ”€â”€ CheckinProcessFunction.java     # Process function with dedup
â”‚       â”‚   â”‚   â””â”€â”€ MySQLCheckinSink.java           # JDBC sink with 2PC
â”‚       â”‚   â””â”€â”€ config/
â”‚       â”‚       â””â”€â”€ JobConfig.java                  # Job configuration
â”‚       â””â”€â”€ resources/
â”‚           â””â”€â”€ log4j2.properties
â””â”€â”€ target/
    â””â”€â”€ loyalty-flink-consumer-1.0.0-SNAPSHOT.jar
```

## ğŸ› ï¸ Build

```bash
mvn clean package
```

Output: `target/loyalty-flink-consumer-1.0.0-SNAPSHOT.jar`

## ğŸš€ Run

### Local Development

```bash
# Run tá»« IDE hoáº·c Maven
mvn exec:java -Dexec.mainClass="vn.ghtk.loyalty.flink.CheckinEventConsumerJob"
```

### Flink Cluster

```bash
# Submit job to Flink cluster
flink run \
  -c vn.ghtk.loyalty.flink.CheckinEventConsumerJob \
  -p 4 \
  target/loyalty-flink-consumer-1.0.0-SNAPSHOT.jar \
  --kafka.bootstrap.servers localhost:9092 \
  --kafka.topic loyalty.checkin \
  --kafka.group.id loyalty-consumer-group \
  --mysql.url jdbc:mysql://localhost:3306/loyalty_db \
  --mysql.username root \
  --mysql.password root \
  --checkpoint.interval 60000 \
  --parallelism 4
```

## âš™ï¸ Configuration

Job arguments (all optional, cÃ³ defaults):

| Argument | Default | Description |
|----------|---------|-------------|
| `--kafka.bootstrap.servers` | `localhost:9092` | Kafka bootstrap servers |
| `--kafka.topic` | `loyalty.checkin` | Kafka topic name |
| `--kafka.group.id` | `loyalty-consumer-group` | Consumer group ID |
| `--mysql.url` | `jdbc:mysql://localhost:3306/loyalty_db` | MySQL JDBC URL |
| `--mysql.username` | `root` | MySQL username |
| `--mysql.password` | `root` | MySQL password |
| `--checkpoint.interval` | `60000` | Checkpoint interval (ms) |
| `--parallelism` | `4` | Job parallelism |

## ğŸ“Š Sizing & Performance

### Quy mÃ´ dá»± kiáº¿n

- **1 triá»‡u event/ngÃ y**: ~12 event/giÃ¢y
- **10 triá»‡u event/ngÃ y**: ~116 event/giÃ¢y
- **50 triá»‡u event/ngÃ y**: ~579 event/giÃ¢y

### Sizing cho 1 triá»‡u event/ngÃ y

- **Kafka Partitions**: 4
- **Flink Parallelism**: 4
- **Checkpoint Interval**: 60 seconds
- **State Size**: ~50 MB (1 triá»‡u event IDs * 50 bytes)
- **Memory per TaskManager**: 2 GB
- **CPU per TaskManager**: 2 cores

### Sizing cho 10 triá»‡u event/ngÃ y

- **Kafka Partitions**: 8
- **Flink Parallelism**: 8
- **Checkpoint Interval**: 60 seconds
- **State Size**: ~500 MB (10 triá»‡u event IDs * 50 bytes)
- **Memory per TaskManager**: 4 GB
- **CPU per TaskManager**: 2 cores

### Sizing cho 50 triá»‡u event/ngÃ y

- **Kafka Partitions**: 16
- **Flink Parallelism**: 16
- **Checkpoint Interval**: 60 seconds
- **State Size**: ~2.5 GB (50 triá»‡u event IDs * 50 bytes)
- **Memory per TaskManager**: 8 GB
- **CPU per TaskManager**: 4 cores

## ğŸ”’ Exactly-Once Guarantees

1. **Kafka â†’ Flink**: Flink Kafka connector vá»›i offset commit sau checkpoint
2. **Flink State**: MapState cho deduplication theo eventId
3. **Flink â†’ MySQL**: JDBC sink vá»›i 2-Phase Commit (XA transactions)
4. **Checkpoint**: RocksDB state backend vá»›i incremental checkpoint

## ğŸ“ Notes

- State TTL: 7 ngÃ y (event IDs auto-expire sau 7 ngÃ y)
- Retry strategy: 3 retries vá»›i exponential backoff
- Timeout: 30 seconds per transaction
- Idempotency: Dá»±a trÃªn unique constraint cá»§a `event_id` trong DB

## ğŸ› Troubleshooting

### Job failed with checkpoint timeout
- TÄƒng `execution.checkpointing.timeout` trong config
- Giáº£m checkpoint interval náº¿u state quÃ¡ lá»›n

### OutOfMemoryError
- TÄƒng heap memory cho TaskManager
- Enable RocksDB state backend cho large state

### MySQL deadlock
- Kiá»ƒm tra index trÃªn `event_id`, `user_id`, `checkin_date`
- TÄƒng MySQL timeout settings

## ğŸ“„ License

Internal project
